---
title: JSON.stringify가 이벤트 루프를 멈추게 하는 이유
date: '2025-10-22T10:44:03.284Z'
tags: ['DEV', 'Node.js', 'Performance']
draft: true
summary: V8·libuv 관점에서 본 "Node.js는 싱글스레드인데 왜 멈칫할까?"
---

## 들어가며

주말에 배포한 API가 이상했다.

모니터링 대시보드를 보니 **p95/p99 응답시간이 갑자기 튀고** 있었다. 평소엔 50ms 정도인데, 어떤 순간에는 3초를 찍는다. DB 쿼리는 빠르고(10ms), 네트워크도 정상인데 도대체 뭐가 문제일까?

```
p50: 50ms  (중앙값, 정상)
p95: 80ms  (상위 5%, 양호)
p99: 3000ms (상위 1%, 급증!) ← 뭐지?
```

로그를 뒤져보니 패턴이 보였다. **대규모 리스트 조회 API**에서만 발생한다. 상품 목록, 주문 내역, 댓글 리스트... 데이터가 많을 때만 느려진다.

"아 DB가 느린가?" 싶어서 쿼리 시간을 찍어봤다. 10ms. 빠르다.

"네트워크?" 아니다. 로컬에서도 재현된다.

범인은 `JSON.stringify()`였다.

20만 개 항목을 한 번에 직렬화하는 순간, **500ms 동안 서버가 먹통**이 되고 있었다. 그 시간 동안 다른 요청들은? 전부 대기. 웹소켓 ping도 지연. 타이머도 안 돌아간다.

"Node.js는 비동기인데 왜?"라고 생각했던 과거의 나에게 말해주고 싶다. **비동기 I/O**랑 **동기 CPU 작업**은 다르다고.

이번 글에서는 이 문제를 겪으며 깨달은 것들을 정리해본다.

---

## TL;DR

- `JSON.stringify()`는 **동기·CPU 바운드** 연산이라 실행되는 동안 **JS 메인 스레드**(= 이벤트 루프가 도는 스레드)를 점유한다. 그 시간만큼 **다른 콜백이 한 틱도 실행되지 못한다**.
- Node.js는 JS 실행을 단일 스레드에서 처리하고, **I/O/일부 네이티브 연산**만 libuv의 **스레드풀(기본 4개)** 로 오프로딩한다. 하지만 `JSON.stringify()` 같은 **순수 JS 실행**은 풀로 가지 않는다.
- 최근 V8이 `JSON.stringify`를 크게 최적화했어도 **동기라는 본질은 동일**하므로, 큰 객체를 한 번에 직렬화하면 여전히 루프 지연이 발생한다. 해결은 **설계적 회피(페이로드 다이어트, 스트리밍, 워커, 멀티프로세스)** 다.

---

**오해**
```
Node.js = 싱글 스레드
→ 모든 게 한 스레드에서 실행
→ 동시에 한 가지 일만 가능
→ 그런데 어떻게 동시 요청 처리?
```

**실제 Node.js**
```
메인 스레드
├─ V8 (JavaScript 실행)
└─ libuv (이벤트 루프)

백그라운드 스레드 (libuv 스레드풀)
├─ 파일 I/O
├─ DNS 조회
├─ zlib/crypto
└─ 기타 네이티브 연산
```

핵심은 이거다. **V8과 libuv는 같은 메인 스레드에서 돌아간다.**

그래서 V8이 바쁘면(예: JSON.stringify 실행 중) libuv도 못 돈다. 이벤트 루프가 멈춘 것처럼 보이는 이유다.

### 이벤트 루프의 단계들

libuv의 이벤트 루프는 여러 단계를 계속 순회한다

```
   ┌───────────────────────────┐
┌─>│         timers            │ ← setTimeout, setInterval
│  └─────────────┬─────────────┘
│  ┌─────────────▼─────────────┐
│  │     pending callbacks     │ ← I/O 콜백 (일부)
│  └─────────────┬─────────────┘
│  ┌─────────────▼─────────────┐
│  │       idle, prepare       │ ← 내부용
│  └─────────────┬─────────────┘
│  ┌─────────────▼─────────────┐
│  │           poll            │ ← 새로운 I/O 이벤트 폴링
│  └─────────────┬─────────────┘
│  ┌─────────────▼─────────────┐
│  │          check            │ ← setImmediate
│  └─────────────┬─────────────┘
│  ┌─────────────▼─────────────┐
│  │      close callbacks      │ ← socket.on('close')
│  └─────────────┬─────────────┘
└────────────────┘
```

각 단계마다 콜백 큐를 확인하고, 있으면 하나씩 실행한다. 근데 **콜백이 오래 걸리면?** 다음 단계로 못 간다.

내 경우가 이랬다

```js
// 타이머 단계에서 실행되는 콜백
function timerCallback() {
  const data = fetchFromCache(); // 1ms - 빠름

  const json = JSON.stringify(hugeData); // 500ms - 여기서 멈춤!

  res.send(json);
}
```

500ms 동안 타이머 단계에 갇혀있으니까:
- poll 단계로 못 감 → I/O 이벤트 처리 지연
- check 단계로 못 감 → setImmediate 지연
- 다른 타이머도 지연

그래서 다른 요청들이 전부 밀린 거였다.

### libuv 스레드풀의 진실

처음엔 이렇게 생각했다. "Node.js는 스레드풀이 있잖아? 거기서 처리하면 되는 거 아냐?"

**아니다.**

libuv 스레드풀이 처리하는 것
- `fs.*` (파일 시스템)
- `dns.lookup()` (DNS 조회)
- `zlib.*` (압축/해제)
- `crypto.*` (암호화)
- C++ 네이티브 바인딩

libuv 스레드풀이 처리 **안** 하는 것
- **JavaScript 함수 실행** ← 여기!
- 순수 JS 계산
- `JSON.stringify()`
- `Array.map()`, `reduce()` 등

왜 JS 실행은 스레드풀로 못 가냐고?

**V8 Heap은 스레드 안전하지 않기 때문이다.** V8의 메모리(Heap)는 단일 스레드 전용으로 설계됐다. 여러 스레드에서 동시에 접근하면 망가진다.

그래서 `UV_THREADPOOL_SIZE`를 아무리 늘려도 `JSON.stringify()` 블로킹은 해결 안 된다. 이걸 깨닫는 데 한참 걸렸다 ㅋㅋ

---

## JSON.stringify는 본질적으로 동기다

ECMAScript 사양을 보면 `JSON.stringify()`의 알고리즘은 **동기적으로** 정의되어 있다.

```
대략적인 흐름:

1. 순환 참조 감지 초기화
2. SerializeJSONProperty(value) 호출
   ↓
3. 타입 확인
   - Primitive? → 문자열 변환
   - Object? → 재귀 순회
   - Array? → 각 요소 재귀
   ↓
4. 각 속성마다
   - 키 직렬화
   - 값 직렬화 (재귀)
   - 이스케이프 처리
   ↓
5. 문자열 결합
   ↓
6. 최종 문자열 반환
```

이 과정 어디에도 `await`가 없다. 중간에 이벤트 루프로 제어권을 양보하지 않는다. 시작하면 끝날 때까지 계속 실행한다.

### V8의 최적화는 "더 빠르게"만 할 뿐

V8은 JSON.stringify를 많이 최적화했다.

Fast Path (빠른 경로) 조건
- 순수 객체/배열 (프로토타입 변경 없음)
- 순환 참조 없음
- toJSON() 메서드 없음
- replacer/space 인자 없음

이 조건을 만족하면 C++ 네이티브 직렬화를 사용한다. JavaScript 구현보다 훨씬 빠르다.

하지만...

```
작은 객체 (1KB):
├─ 최적화 전: 0.5ms
└─ 최적화 후: 0.1ms (5배 빠름!)

큰 객체 (10MB):
├─ 최적화 전: 500ms
└─ 최적화 후: 100ms (5배 빠름!)

→ 여전히 100ms 블로킹!
```

빠르긴 한데, **동기라는 본질은 그대로**다. 결국 큰 데이터를 한 번에 직렬화하면 블로킹은 피할 수 없다.

---

## 블로킹의 순간을 시각화해보자

내 상황을 타임라인으로 그려보면 이랬다.

```mermaid
sequenceDiagram
    participant Timer as Timers 단계
    participant V8 as V8 (메인 스레드)
    participant Loop as Event Loop
    participant OS as OS/네트워크

    Note over V8: HTTP 요청 도착
    Timer->>V8: 타이머 콜백 실행
    V8->>V8: DB 조회 (10ms) - 빠름

    V8->>V8: JSON.stringify(big) 시작

    Note over V8,Loop: 메인 스레드 점유<br/>이벤트 루프 정지!

    OS-->>Loop: 다른 I/O 완료 신호
    Note over Loop: 처리 못 함 (V8 바쁨)

    V8->>V8: JSON.stringify 계속...
    V8->>V8: 완료 (500ms 후)

    V8->>Timer: 콜백 완료
    Timer->>Loop: 다음 단계로

    Note over Loop: 이제야 밀린 이벤트 처리
    Loop->>V8: poll 단계 실행
```

**핵심 포인트**
- 메인 스레드가 stringify 동안 **묶여있다**
- 그 시간만큼 타이머/소켓/파일 I/O 콜백이 **밀린다**
- 백그라운드에서 I/O는 완료되지만 **콜백 실행은 못 함**

그래서 p99가 튀는 거였다. 동시 요청이 들어오면?

```
요청 1: [─ 10ms ─][━━━━ 500ms ━━━━][─ 10ms ─] 완료 (520ms)
요청 2:                               [─ 10ms ─][━━━━ 500ms ━━━━] 완료 (1030ms)
요청 3:                                                            [─ 10ms ─][━━━━...

→ 순차 처리, p99 폭발
```

---

## 문자열 불변성이 비용을 2배로 만든다

JavaScript 문자열은 **불변(immutable)** 이다. 이게 큰 문제다.

```js
let str = "hello";
str[0] = "H";  // 작동 안 함
console.log(str);  // "hello" (그대로)

// 새로 만들어야 함
str = "H" + str.slice(1);  // 새 문자열 생성
```

JSON.stringify가 문자열을 만들 때도 마찬가지다.

```js
// 의사코드

result = "";
result += "{";                    // 새 문자열 생성 (2 byte)
result += '"id":';                // 새 문자열 생성 (7 byte), 기존 복사
result += "1";                    // 새 문자열 생성 (8 byte), 기존 복사
result += ",";                    // 새 문자열 생성 (9 byte), 기존 복사
result += '"name":"test"';        // 새 문자열 생성 (22 byte), 기존 복사
result += "}";                    // 새 문자열 생성 (23 byte), 기존 복사
```

**매 `+=` 연산마다**
1. 새 메모리 할당
2. 기존 문자열 **전체 복사**
3. 새 내용 추가
4. 기존 메모리 해제 (GC 대상)

시간 복잡도가 O(n²)에 가까워진다. n개 속성을 직렬화하면:

```
1번째: 10 byte 복사
2번째: 20 byte 복사
3번째: 30 byte 복사
...
n번째: n×10 byte 복사

총: 10 + 20 + 30 + ... + n×10 = O(n²)
```

20만 개 항목이면? 어마어마한 복사 비용이다.

### V8의 Rope 최적화

V8은 이걸 **Rope**(로프) 자료구조로 완화한다.

```
일반 문자열 결합:
"hello" + " " + "world"
→ "hello" 복사 → "hello " 생성
→ "hello " 복사 → "hello world" 생성

Rope:
"hello" + " " + "world"
→ 트리 구조로 참조만 유지
    [Concat Node]
       /      \
    [hello]  [Concat Node]
                /      \
              [ ]    [world]

→ 필요할 때만 평탄화(flatten)
```

장점은 문자열 결합이 O(1)이 된다는 것. 하지만 최종적으로는 평탄화가 필요하고(네트워크 전송 시), Rope 자체도 메모리 오버헤드가 있다.

내가 겪은 500ms 블로킹은 이 평탄화 과정도 포함된 시간이었다.

---

## 실전 회피 전략

이제 해결책을 찾아야 했다. 여러 방법을 시도해봤다.

### 전략 1: 직렬화량 자체를 줄이기 (가장 효과적)

제일 먼저 한 건 **불필요한 데이터 제거**였다.

```js
// Before (내부 필드까지 전부)
{
  id: 1,
  name: "상품A",
  price: 10000,
  _internal_id: "abc123",        // 클라이언트 불필요
  _debug_info: {...},            // 개발용
  _sql_metadata: {...},          // DB 메타데이터
  created_at: "2024-01-01",
  updated_at: "2024-01-01",
  deleted_at: null,              // 거의 null
  legacy_field_1: null,
  legacy_field_2: null
}

// After (필요한 것만)
{
  id: 1,
  name: "상품A",
  price: 10000
}
```

이것만으로 **직렬화 시간이 60% 감소**했다. 500ms → 200ms.

replacer 함수도 썼다.

```js
JSON.stringify(obj, (key, value) => {
  // 내부 필드 제외
  if (key.startsWith('_')) return undefined;

  // null 필드 제외
  if (value === null) return undefined;

  return value;
});
```

간단하지만 효과는 확실했다.

### 전략 2: NDJSON 스트리밍

두 번째로 시도한 건 **스트리밍**이다.

거대한 배열을 한 번에 JSON으로 만들지 말고, **줄 단위로 쪼개서** 보낸다.

```
NDJSON 형식 (Newline Delimited JSON):

{"id":1,"name":"A"}
{"id":2,"name":"B"}
{"id":3,"name":"C"}

각 줄이 독립된 JSON
줄바꿈(\n)으로 구분
```

장점
- 배열 시작/끝 괄호(`[]`) 불필요
- 각 줄은 작은 JSON → 빠른 직렬화
- 즉시 전송 가능 → 블로킹 분산

내 경우 이렇게 바꿨다.

```js
// Before (메모리에 전부)
const items = await db.getItems();
const json = JSON.stringify(items);  // 500ms 블로킹
res.send(json);

// After (스트리밍)
const itemStream = db.getItemStream();

for await (const item of itemStream) {
  const line = JSON.stringify(item) + '\n';
  res.write(line);  // 즉시 전송
}
res.end();
```

효과
```
메모리: 50MB → 1MB (한 줄 크기만)
블로킹: 500ms (한 번에) → 0.5ms × 200,000 (분산)
→ 각 블로킹이 짧아서 체감 없음
```

클라이언트는 줄 단위로 파싱하면 된다. 라이브러리도 많다.

### 전략 3: 멀티프로세스

세 번째는 **프로세스를 늘리는 것**이다.

CPU 코어가 8개인데 프로세스는 1개만 쓰고 있었다. 나머지 7개는 놀고 있다니!

```
단일 프로세스:
CPU 0: [████████] 100% ← Node.js
CPU 1: [        ]   0%
CPU 2: [        ]   0%
...
CPU 7: [        ]   0%

활용률: 12.5% (1/8)
```

PM2로 클러스터 모드를 켰다.

```js
// ecosystem.config.js
module.exports = {
  apps: [{
    name: 'api-server',
    script: './server.js',
    instances: 'max', // CPU 코어 수만큼
    exec_mode: 'cluster'
  }]
};
```

```bash
pm2 start ecosystem.config.js
```

효과

```
8 프로세스:
CPU 0: [████████] 100%
CPU 1: [████████] 100%
CPU 2: [████████] 100%
...
CPU 7: [████████] 100%

활용률: 100%
```

동시 요청 처리
```
Before (단일):
동시 처리: 1요청
처리량: 2 RPS

After (8개):
동시 처리: 8요청
처리량: 16 RPS

→ 8배 향상!
```

한 요청의 블로킹 시간은 그대론데, **전체 처리량**이 올라갔다. p99도 안정됐다.

### 전략 4: Worker Threads (고려했지만)

Worker Threads도 생각했다. JSON.stringify를 별도 스레드로 보내는 거다.

```
메인 스레드:
├─ HTTP 요청 수신
├─ DB 쿼리
├─ Worker에 데이터 전송
├─ 다른 요청 처리 (블로킹 없음!)
└─ Worker 결과 수신 → 응답

Worker 스레드:
├─ 데이터 수신
├─ JSON.stringify (여기서 블로킹)
└─ 결과 전송
```

장점은 메인 스레드가 안 막힌다는 것.

근데 **데이터 전달 비용**을 고려해야 한다.

```
복사 비용:

작은 데이터 (1KB):
├─ 복사: 0.1ms
├─ stringify: 0.1ms
└─ 결론: 워커 불필요 (오버헤드)

큰 데이터 (10MB):
├─ 복사: 100ms
├─ stringify: 500ms
└─ 결론: 워커 효과적? (애매)
```

내 경우는 데이터가 너무 커서 복사 비용도 만만치 않았다. 그래서 **스트리밍 + 멀티프로세스** 조합으로 갔다.

---

## 측정과 모니터링

문제를 해결하고 나서, **지속적인 모니터링**을 설정했다.

Node.js의 `perf_hooks`로 이벤트 루프 지연을 측정한다.

```js
import { monitorEventLoopDelay } from 'node:perf_hooks';

const h = monitorEventLoopDelay({ resolution: 20 });
h.enable();

setInterval(() => {
  console.log(`
    평균: ${(h.mean / 1e6).toFixed(1)}ms
    p95: ${(h.percentile(95) / 1e6).toFixed(1)}ms
    p99: ${(h.percentile(99) / 1e6).toFixed(1)}ms
  `);
  h.reset();
}, 10000);
```

정상 vs 블로킹 비교

```
정상 (최적화 후):
평균: 0.5ms
p95: 2ms
p99: 5ms

블로킹 (최적화 전):
평균: 50ms
p95: 200ms
p99: 500ms
```

이제 p99가 10ms를 넘으면 알림이 온다. 새로운 블로킹이 생기면 바로 잡을 수 있다.

---

## 마무리하며

이 문제를 겪으며 깨달은 것들을 정리하면:

**1. "비동기" ≠ "블로킹 없음"**

Node.js는 I/O가 비동기일 뿐, CPU 작업은 여전히 동기다. JSON.stringify, 암호화, 이미지 처리... 이런 건 메인 스레드를 점유한다.

**2. V8과 libuv는 같은 스레드**

V8이 바쁘면 libuv도 못 돈다. 이벤트 루프가 "멈춘 것처럼" 보이는 이유다.

**3. libuv 스레드풀 ≠ JS 실행 풀**

스레드풀은 네이티브 I/O용이다. JS 함수 실행은 메인 스레드에서만 돈다.

**4. 최적화 우선순위**

```
1. 직렬화량 줄이기 (가장 효과적)
2. 스트리밍 (블로킹 분산)
3. 멀티프로세스 (처리량 향상)
4. Worker Threads (복사 비용 고려)
```

**5. 측정이 먼저**

최적화하기 전에 **측정**부터 해야 한다. 어디가 느린지 모르면 뭘 고칠지도 모른다.

처음엔 막막했는데, 하나씩 이해하고 나니 해결책이 보였다. 여러분도 비슷한 문제를 겪고 있다면 도움이 되길 바란다.

---

## 참고: HTML 직렬화도 같은 문제

여담인데, 이 문제는 서버사이드 렌더링(SSR)의 **HTML 직렬화**와 본질적으로 같다.

둘 다 **메인 스레드에서 동기적으로 거대한 문자열을 만드는 작업**이고, 수행 동안 이벤트 루프가 멈춘다.

```
공통점:
├─ 메인 스레드 점유
├─ 이벤트 루프 블로킹
├─ CPU 바운드
└─ 문자열 불변성 문제

차이점:
HTML이 더 무겁다
├─ 컴포넌트 함수 실행
├─ 가상 DOM 생성
└─ 마크업 변환
```

해법도 비슷하다.

```
JSON:
├─ 페이로드 다이어트
├─ 스트리밍 (NDJSON)
├─ 워커/멀티프로세스

HTML:
├─ 페이로드 다이어트 (불필요한 컴포넌트 제거)
├─ 스트리밍 (renderToPipeableStream)
├─ 멀티프로세스 + ISR/정적 생성
```

> **한 줄 정리**: 큰 **동기 문자열 생성**(JSON이든 HTML이든)은 모두 이벤트 루프의 적이다. "작게, 나눠서, 미리/다른 곳에서" 만들어라.

상세한 내용은 [다음 글: HTML 직렬화도 이벤트 루프를 멈춘다](/blog/html-serialize)에서 다룬다.

---

## 참고 자료

- [Node.js Event Loop 공식 문서](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/)
- [libuv 설계 문서](http://docs.libuv.org/en/v1.x/design.html)
- [V8 String 최적화](https://v8.dev/blog/string-optimizations)
- [JSON.stringify ECMAScript 사양](https://tc39.es/ecma262/#sec-json.stringify)
