---
title: 휴톰 프론트엔드 파트 MonoRepo  도입
date: '2024-05-23T10:44:03.284Z'
tags: ['DEV']
draft: true
summary: 흩어져있는 프론트엔드 한곳으로 뭉치기
---

# **도입 배경**

안녕하세요. 휴톰 프론트엔드 개발자 박준형입니다.

휴톰의 프론트엔드 제품은 각자의 Repository를 갖고있는 멀티레포 방식으로 관리되고있습니다. 각각의 제품은운영방식과 라이프사이클이 모두 다르게 관리되고있습니다.

![](/static/images/DEV/hutom-monorepo/hutom-frontend-repository.png)

독립적인 관리포인트를 갖는 멀티레포 방식은 프로젝트의 완전한 독립성을 보장한다는 장점을 갖고있습니다. 하지만 각각의 관리포인트를 갖고있기 때문에 개발자들의 불편함도 함께 다가오고있다고 우려되었습니다.

물론 긴밀한 의사소통과 미팅과 같은 추상적인 공유를 통해 관리포인트의 문제를 해결할 수 도 있습니다. 하지만 물리적으로 공유도 하며 추상적인 공유도 함께한다면 의사소통의 비용도 절감되지않을까라는 생각을 하게되었습니다.

예를 한번 들어보겠습니다.

`hVAT`와 `hServer` 는 이미 출시가 되어있는 서비스입니다. 신규 서비스인 `SurgGram` 를 만들어야하는 상황에서 프로젝트를 빌딩하는 과정을 한번 생각해봅니다. 만약 `SurgGram`의 특정기능이 `hVAT`에서 사용하고있는 비디오 플레이어의 기능과 유사하다고 판단되어 컴포넌트와 여러 헬퍼함수 모듈을 개발자가 직접 코드를 복사하고 붙여넣기를 하는 작업을 수행하게 됩니다. 이런 경우 직접적인 공유가 아닌 간접적인 공유입니다.

제품마다 배포전략과 브랜치 전략을 픽스하며 문서로 마련하고, 이러한 브랜치 전략 까지 하나의 코드베이스에서 관리를 한다면, 전략이 표준화되고 업무 프로세스까지 마련되어 정신적인 스트레스또한 사라지지 않을까하여 모노레포를 도입하게 되었습니다.

## 모노레포를 도입하여 얻게되는 이점

- 비슷한 제품군을 하나의 저장소에서 관리
  - 공통 모듈, 헬퍼, 유틸 함수를 함께 사용이 가능합니다.
  - 서로 의존적인 패키지의 리팩토링 비용감소
- 신규 제품 프로젝트를 생성할때마다 발생하는 오버헤드 절감
  - 레포지토리 생성 → 커미터 생성 → 개발환경 세팅 → CI/CD 구축 → 빌드 → publish 과정생략
- 관리포인트가 단일화
- 코드스타일 통일화
  - 커미터 생성에서 린트 및 프리티어 설정을 하는데 레포지토리 기준으로 룰을 정하니 스타일 통일화 기대할수있다.
  - 한조직이 개발했지만 한 사람이 개발한것과 같은 퀄리티의 코드스타일 통일성을 기대할수 있습니다.

## 모노레포를 도입하여 우려되는 점

- 기존에 사용중인 **GitFlow** 브랜치 전략 사용 불가
  - 서비스마다 develop브랜치와 main 브랜치를 만들기에는 현실적으로 어려움
- 테스트 및 빌드 범위
  - 소스 변경시 모든 프로젝트를 다시 빌드하거나 다시 테스트하지 않으며 영향을 받는 프로젝트만 다시 테스트하고 빌드해야한다.

도입하며 얻게되는 이점과 우려지점은 도구를 선택한뒤 해결하기로하고, 우선 어떤 패키지 매니저와 모노레포 툴을 사용할지 고려를 하게되었습니다.

# 패키지 매니저 비교

모노레포를 도입하며 패키지매니저도 바꾸어보자는 생각과 함께 비교를 해보기로 했습니다.

기존의 프론트엔드에서 패키지매니저로 Yarn Classic을 사용했습니다. Yarn Classic이란 Yarn 1.x 버전을 지칭합니다. 성능면에서 다소 느릴 뿐만 아니라 의존성 중복저장 문제를 해결하기위해 호이스팅 방법을 사용합니다. 그렇다면 어떤 문제가 존재하는지 알아야합니다.

![](/static/images/DEV/hutom-monorepo/pnpm.png)

- node_modules의 문제점

  - node.js에서 require 함수를 실행하면 모듈을 찾을때까지 상위 디렉토리를 순회하는데 이때 느린 Disk I/O 동작이 경로의 깊이만큼 발생하게 됩니다.

- 저장공간과 설치시간
  - node_modules는 매우 큰 공간을 차지한다. 용량만 많이차지한다면 다행입니다. 디렉토리 구조를 만들기위해서 많은 I/O 작업이 필요합니다.
  - 복잡한 폴더구조이기때문에 설치가 유효한지 검증하기 어렵습니다. 수백개의 패키지가 서로를 의존하는 복잡한 의존성 트리에서 디렉토리 구조는 깊어집니다.
  - 디스크 I/O 호출은 메모리의 자료구조를 다루는것보다 훨씬 느립니다.
- 유령의존성
  - 왼쪽 트리에서 A와 C패키지는 B패키지를 의존성을 띄게 되는데 A 와 C를 설치하며 B를 두번 설치하게되어 디스크 공간 낭비를 하게됩니다.
    - 이때 package-1에서는 직접 require(’B’) 가 불가합니다.

![](/static/images/DEV/hutom-monorepo/hoist.png)

- 이를 해결하기위해 오른쪽트리처럼 원래의 트리의 모양을 바꿉니다.
  - 이렇게 되면서 package-1에서 직접 require(’B’)가 가능해졌습니다.
  - 직접의존하지않는 라이브러리를 require할수있는 현상을 유령 의존성이라고합니다.
  - 이는 package.json**에서 제거했을때 소리없이 사라지며 의존성관리 시스템을 혼란스럽게 만듭니다.**

이를 해결할 수 있는 방법은 크게 두가지입니다. Yarn Berry와 Pnpm입니다. Yarn Berry는 Plug’n Play 전략을 통해 중복 저장 문제를 해결하며 호이스팅 방법을 사용하지않는것이 기본값입니다. 하지만 저희가 선택한 패키지매니저는 Pnpm입니다.

선택한 이유부터 말하면, 뒤에서 선택하게된 **TurboRepo**에서는 현재까지 Yarn Berry의 Plug’n Play을 지원하지 않기 때문입니다. Pnpm이라는 선택지가 남았으며, Pnpm의 매력을 찾기로했습니다.

## 패키지 매니저 벤치마킹

아래의 테이블은 pnpm 문서에서 제공되는 패키지파일 설치 속도 비교 벤치마킹 입니다

![](/static/images/DEV/hutom-monorepo/bench-mark.png)

cache가 있는경우에는 초록색, 없는 경우에는 노란색으로 구분했습니다. Yarn(PnP)는 선택지에 없으므로 배제하고 보시면됩니다.

각각의 케이스 별로 보더라도 pnpm을 사용한 사례가 성능상 빠르기때문에 좋은 선택지가 될 수 있다고 생각했습니다.

그렇다면 Pnpm은 중속성 중복 저장 문제를 잘 해결하는지가 궁금합니다.

pnpm의 node_modules 레이아웃은 심볼릭 링크를 사용하여 중첩된 종속성 구조를 만듭니다. node_modules 내의 모든 패키지의 모든 파일은 [Content-addressable Store](https://pnpm.io/next/symlinked-node-modules-structure) 에 대한 하드링크입니다. bar@1.0.0에 종속된 foo@1.0.0을 설치한다고 가정했을때 하기 구조처럼 하드링크합니다.

```markdown
node_modules
└── .pnpm
├── bar@1.0.0
│ └── node_modules
│ └── bar -> <store>/bar
│ ├── index.js
│ └── package.json
└── foo@1.0.0
└── node_modules
└── foo -> <store>/foo
├── index.js
└── package.json
```

위의 구조는 실제로 설치되는 node_modules의 형태입니다. 그래프 구조를 구축하기 위해 심볼릭 링크가 생성됩니다.

두 패키지는 모두 node_modules 폴더내의 하위 폴더에 하드 링크 되어 있습니다. 이러한 하드링크구조는 패키지가 스스로를 가져올 수 있도록 허용합니다. foo는 require(’foo/pacakage.json') or import \* as package from “foo/package.json”로 가져올 수 있어야합니다.

다음은 종속성 심볼릭 링크로 bar는 foo@1.0.0/node_modeuls에 심볼릭 링크됩니다.

```markdown
node_modules
└── .pnpm
├── bar@1.0.0
│ └── node_modules
│ └── bar -> <store>/bar
└── foo@1.0.0
└── node_modules
├── foo -> <store>/foo
└── bar -> ../../bar@1.0.0/node_modules/bar
```

다음으로 직접 종속성을 처리합니다. foo는 프로젝트의 종속성이므로 루트 node_modules 폴더에 심볼릭 링크됩니다.

```markdown
node_modules
├── foo -> ./.pnpm/foo@1.0.0/node_modules/foo
└── .pnpm
├── bar@1.0.0
│ └── node_modules
│ └── bar -> <store>/bar
└── foo@1.0.0
└── node_modules
├── foo -> <store>/foo
└── bar -> ../../bar@1.0.0/node_modules/bar
```

결과적으로 Pnpm를 사용하면 모든 종속성은 전역 스토어에 물리적으로 한번만 저장되어 SSOT(단일 진실 공급원)을 구성하며 디스크 공간을 절약하게 됩니다.

# 터보레포

TurboRepo의 주요 미션은 모노레포 환경에서 개발자가 조금 더 쉽고 빠르게 개발할 수 있도록 빌드 도구를 제공하는것입니다.

## Content-aware-hashing

- 타임스탬프가 아닌 콘텐츠를 인식하는 방식으로 해싱을 지원합니다.
- 모든 파일을 다시 빌드하는게 아니라 변경된 파일만 빌드합니다.

## Pruned subsets

모노레포는 락파일을 전역으로 관리합니다. reserach-db 워크스페이스와 surggram 워크스페이스가 있습니다. surggram의 디펜던시가 변경된다면 전역 락파일에 반영됩니다.

디펜던시가 변경되지않은 research-db 워크스페이스를 도커로 빌드할때 락파일이 변경된것을 보고 새로 디펜던시를 설치하기 시작할것입니다.

락파일을 전역으로 사용하기 때문에 문제라면 빌드할 워크스페이스만 떼어내 락파일을 도커에게 전달해주면 편하지않을까요?

```markdown
├── apps
| ├── research-db // design-system, research-db-oas 사용
| | ├── src/..
| | ├── package.json
| |
| ├── surggram // design-system, surggram-oas 사용
| ├── src/..
| ├── package.json
|
├── packages
| ├── design-system
| | ├── src/..
| | ├── package.json
| |
| ├── research-db-oas
| | ├── src/..
| | ├── package.json
| |
| ├── surggram-oas
| ├── src/..
| ├── package.json
|
├── pnpm-lock.yaml // 전역 락 파일!
```

위와 같은 구조로 되어있다고 가정하고 surggram을 빌드해야하는 상황이라고 가정해봅시다. surggram 패키지는 design-system과 surggram-oas 패키지만 사용중입니다.

터보레포를 사용하지않고 workspace 필드만을 사용해서 Docker 이미지를 만들어야하는 상황을 생각해봅시다.

**Dockerfile (Prune 사용 X)**

```docker
FROM node:18-alpine AS base

FROM base AS builder
WORKDIR /app
COPY . .

FROM base AS installer
WORKDIR /app

# 필요한 패키지들을 복사
COPY --from=builder /app/apps/surggram/ . ./apps/surggram
COPY --from=builder /app/package/design-system/ . ./package/design-system
COPY --from=builder /app/package/surggram-oas/ . ./package/surggram-oas

# 여기서 문제 발생!
COPY --from=builder /app/out/pnpm-lock.yaml ./pnpm-lock.yaml

# 패키지 설치
RUN pnpm install

# Build the project
WORKDIR /app/apps/surggram
RUN pnpm build

# Stand Alone 설정으로 Runner Stage
FROM base AS runner

# 중간 생략

CMD node apps/surggram/server.js
```

어찌저찌해서 필요한 패키지들을 빌드와 인스톨 스테이지에 모아왔다고 가정해봅시다. 하지만 모노레포 빌드 환경에서 락파일은 전역에 존재합니다. 락파일은 가지치기가 되어있지않아 빌드타겟인 SurgGram 패키지의 의존성만 설치되는것이 아니라 불필요한 의존성까지 설치될것입니다.

이는 또 잠재적으로 도커 이미지 빌드에 사용될 워크스페이스와 관련 없는 의존성 변경으로 `캐시 miss`를 야기할수도있습니다.

이런문제를 터보레포도 진작에 파악하고있고 **Prune**이라는 기능을 제공해줍니다.

`turbo prune --scope="surggram”` 라고 CLI 명령어를 입력하면 아래와 같은 결과물을 얻습니다.

```markdown
├── apps
| ├── surggram // design-system, surggram-oas 사용
| ├── src/..
| ├── package.json
|
├── packages
| ├── design-system
| | ├── src/..
| | ├── package.json
| |
| ├── surggram-oas
| ├── src/..
| ├── package.json
|
├── pnpm-lock.yaml // 가지치기된 전역 락 파일!
```

무엇이 달라졌는지 눈에 보이시나요? SurgGram과 관련된 패키지들만 가지치기가 되었습니다. 하지만 제가 우려했던것은 전역 락파일을 불필요한 의존성 문제였죠?

아래의 사진은 전역 락파일과 prune 기능으로 만들어낸 가지치기된 락파일입니다.

![](/static/images/DEV/hutom-monorepo/lockfile.png)

무려 900줄이나 차이가 존재합니다. 이로써 SurgGram과 관련된 패키지들만 락파일에 생성이 되었다는것입니다.

더욱 매력적인것은 —docker 옵션입니다. 이 옵션을 사용하면 아래와 같이 뽑아줍니다.

```txt
├── full // 집중!!
| ├── apps
| | ├── surggram
| | ├── src/..
| | ├── package.json
| |
| ├── packages
| ├── design-system
| | ├── src/..
| | ├── package.json
| |
| ├── surggram-oas
| ├── src/..
| ├── package.json
|
├── json // 집중!!
| ├── apps
| | ├── surggram
| | ├── package.json
| |
| ├── packages
| ├── design-system
| | ├── package.json
| |
| ├── surggram-oas
| ├── package.json
|
├── pnpm-lock.yaml
```

full 디렉터리에서 package.json 파일만을 가져와서 만들어진 json 디렉터리라는점이 눈에 보입니다.

`Install Stage(JSON) → Build Stage(Full) → …`

이제 Install Stage에서 의존성 설치에 필요한 pnpm-lock.yaml과 package.json 만을 가질수 있게 되었고, 더이상 의존성과 관련되지않은 파일의 변경으로 캐시가 miss되는 일은 생기지 않을것입니다.

## Cloud caching

- 클라우드 빌드 캐시를 팀원 및 CI/CD의 환경인 Github actions와 공유합니다.
- 로컬환경을 넘어 클라우드 환경에서도 빠른 빌드를 제공합니다.

아래의 사진은 각각 컴퓨터에서 캐시가 로컬로 저장되는 상황입니다. 이를 클라우드 캐시로 올려서 불필요한 중복작업을 제거 해볼 수 있습니다. 터보레포는 기본적으로 로컬 파일 시스템에만 캐시를 저장하므로 모든 작업 입력이 동일하더라도 동일한 작업(빌드, 린트, 테스트)을 각 머신에서 다시 실행해야 하며, 이는 명백한 리소스 낭비입니다.

![](/static/images/DEV/hutom-monorepo/local-cache.png)

팀원들과 그리고 CI환경에서도 하나의 캐시를 공유할 수 있다면 조직 전체에서 중복 작업을 방지하여 시간을 절약할것입니다.

![](/static/images/DEV/hutom-monorepo/remote-cache.png)

### 리모트 캐시 API 제작

터보레포에서는 리모트 캐시를 사용하기 위해서는 API 사양을 충족하는 모든 HTTP 서버에서 구현할 수 있다고 안내합니다. Vercel에서 제공하는 원격캐시를 사용할 수 도있지만,
굳이 Pro나 Enterprise 요금제를 사용하기가 껄끄럽기도하며 병원 및 환자의 데이터의 안전을 위해 안전한 API 서버를 만들기로 합니다.

[turborepo-remote-cach](https://github.com/ducktors/turborepo-remote-cache)에서 제공해주는 소스코드를 사용하면됩니다.

![](/static/images/DEV/hutom-monorepo/supported-storage-provider.png)

fastify를 기반으로 구성되어 미리 구현되어있는 엔드포인트를 통해 따로 작업없이 서버를 띄우는것만으로도 원격캐싱이 지원되며, 현재 휴톰에서 사용중인 **Google Cloud Storage**서비스를 사용할 수 있습니다.

커스텀 캐시 서버를 구축하는 과정은 다음과 같습니다.

1. [turborepo-remote-cach](https://github.com/ducktors/turborepo-remote-cache)에서 소스코드를 클론합니다.
2. Storage-Provider를 위한 설정 및 기타 환경변수를 입력합니다.
3. Docker Image를 만들어 Cloud Run과 같은 서버리스 호스팅 서비스에서 제공합니다.

커스텀 캐시 서버를 구축했으니 커스텀 캐시를 사용하는 방법은 굉장히 간단합니다.

Turborepo의 루트 디렉토리에 `.turbo/config.json` 파일을 생성하여 아래와 같은 정보를 입력합니다.

- teamid : 커스텀 캐시 서버의 teamid를 기입합니다.
- token : 커스텀 캐시 서버의 token를 기입합니다.
- apiurl : 커스텀 캐시 서버의 Url을 기입합니다.

![](/static/images/DEV/hutom-monorepo/result-remote-cache.png)

마지막 줄을 보면 `Remote caching enabled`라는 문구가 있습니다! 이로서 리모트캐시를 제공하는 http 서버는 생성했습니다.

# 버전 관리

휴톰에서 만들어지고 있는 제품들을 이해하기 위해서는 제품의 버전을 먼저 이해해야합니다! 우리는 한 제품을 만들기위해 여러부서와 버전의 컨텍스트를 공유합니다.

제품의 통일된 버전 정책을 가지기 위해서는 유의적인 버전을 사용해야합니다. [유의적 버전](https://semver.org/lang/ko/)이란 제품에 버전을 어떻게 부여하고 관리해야하는지에 대한 규칙입니다.

휴톰에서는 유의적버전을 사용하지만 여러 유관부서와 협업을 해야하기때문에 **pre-release-version**과 **pre-release-code**가 일반적인 유의적버전과는 다르게 사용하고있습니다.

![](/static/images/DEV/hutom-monorepo/version.png)

- **major**, **minor**, **patch** 은 유의적버전과 동일하게 사용합니다 🙂
  - **major**, **minor**, **patch**는 PM팀에서 정해주어 모든 부서가 공유하는 버전입니다.
- **stage**는 구현단계와 시스템 테스트 단계에서 주로 사용합니다.
  - 구현단계 : **patch** 버전 뒤에 `-dev` 를 붙여 사용하며 몇번째 단계인지 숫자로 표기합니다.
    - ex) `v1.0.0-dev1`, `v1.0.0-dev2`, `v1.0.0-dev3`
  - 시스템 단계 : V&V팀에서 테스트를 진행하는 단계로 구현단계와 마찬가지의 컨벤션을 따릅니다.
    - ex) `v1.0.0-vnv1`, `v1.0.0-vnv2`
  - **stage**가 반복적으로 이루어지는 경우 **stage**뒤에 숫자를 붙여 사용합니다.
- **build**는 PLT 팀이 **stage**에서 몇번째 빌드했는지를 명시합니다.
  - **build** 정보는 자연수로 표기하며 **stage**가 변경되어도 **build** 숫자는 초기화 하지 않습니다.
  - ex) `v1.0.0-vnv1-build1`, `v1.0.0-vnv1-build2`, `v1.0.0-vnv2-build3`

# 브랜치 전략 비교

## Git Flow

![](/static/images/DEV/hutom-monorepo/git-flow.png)

위의 그림은 기존에 사용하던 GitFlow 전략입니다. 휴톰의 프로젝트는 각각 배포 주기가 다를수도있으며, 운영방식이 다르기 때문에 이에 맞춰 유연하게 사용할 수 있어야합니다.

### Git Flow 는 왜 모노레포에서 사용하기 어려운가

**모노레포에서는 GitFlow로는 복잡성 문제를 해결하기 어렵습니다.**

- 복잡성 증가
  - 모노레포는 여러 프로젝트를 하나의 저장소에서 관리하기 때문에 복잡합니다. GitFlow는 브랜치 관리전략이 복잡한데 여러 프로젝트의 다양한 브랜치를 관리하기에는 부적절합니다.
- 동기화 문제
  - GitFlow는 feat, develop, main 브랜치 등을 통해 작업을 합니다. 모노레포에서는 여러 프로젝트의 브랜치가 서로 연관되어 서로 영향을 줄 수 있어 동기화가 어렵습니다.
- 릴리즈 관리의 어려움
  - 모노레포에서는 여러 프로젝트가 동시에 릴리즈 되는 경우가 많습니다. GitFlow는 개별 프로젝트의 릴리즈를 관리하는데는 적합하지만 여러 프로젝트를 관리하는 모노레포에서는 릴리즈를 조율해야 하므로 릴리즈 병목이 발생할 수 있습니다.

## Trunk-Based Development

![](/static/images/DEV/hutom-monorepo/tbd.png)

**Trunk-Based Development** 란 소스 제어 도구에서 브랜치를 어떻게 관리할지에 대한 전략입니다. 개발자들은 마스터에서 협업하고, 긴 작업 단위의 브랜치 생성을 피해야합니다. 그렇게 함으로서 협업 간 충돌을 최소화하고 빌드가 실패되는 상태를 막습니다.

이 모델의 가장 주요한 특징은 트렁크라 불리는 마스터 브랜치를 항상 배포 가능한 올바른 상태로 유지하는 것이고 이를 보장하기 위한 주요 규칙은 다음과 같습니다.

### **개발규칙**

- 개발은 브랜치 푸시로 진행합니다.
  - 브랜치 푸시: 마스터에서 파생된 피쳐 브랜치를 생성한 후 PR을 통해 마스터로 푸시합니다.
- 휴톰 프론트엔드 파트에서는 개발 중 직접 푸시를 사용하지 않고, 브랜치 푸시를 사용합니다.
- 피쳐 브랜치는 가능한 작은 작업 단위를 가져야 합니다.
- 장기간 지속하여야 하는 피쳐 개발의 경우 피쳐 플래그와 인터페이스를 사용해 작업 단위를 나눕니다.

### **배포 규칙**

- 배포는 브랜치 배포로 진행합니다.
  - 브랜치 배포: 마스터에서 필요 시점에 배포 브랜치를 생성 후 배포를 수행합니다.
- 휴톰 프론트엔드 파트에서는 필요에 따라 직접 배포와 브랜치 배포를 혼용하고 있습니다.
- 배포 브랜치에서 작업한 결과는 마스터로 머지하지 않습니다.
- 배포 브랜치에서 추가 작업이 필요할 경우 마스터에서 작업을 수행한 후 해당 커밋을 체리픽하여 가져옵니다.

프로젝트마다 배포 주기가 다를수도있으며, 운영방식이 다르기 때문에 이에 맞춰 유연하게 사용할 수 있어야합니다.

이에따라 휴톰의 프론트엔드 파트는 기존의 **GitFlow**를 버리고 새롭게 **Trunk Based Development(TBD)**로 해결해보려합니다.

이를 위해 Feature Flag를 먼저 도입해야합니다.

## Feature Flag

사용자에게 보이는 기능을 동적으로 토글 할 수 있는 스위치로 서비스 중인 앱에 새로운 기능을 추가하거나 변경할 때 그 변경사항을 일부 사용자에게만 보이게 하거나 숨기는데 사용할 수 있습니다.

피처플래그의 보편적인 사용방법은 사용자들에게 실험적 혹은 제한된 기능을 제공하는데 사용됩니다.

애플리케이션에는 [버전](https://www.notion.so/204c8fae53284353ad47dc89a69e886d?pvs=21)이 존재합니다. 피처플래그가 제한된 기능을 제공한다는것에서 영감을 받아 버전에따라 기능의 범위를 제한하도록 피처플래그를 설계해보는 방법을 떠올리게되었습니다.

### 버전에 따라 기능제공의 여부를 선택하도록 Feature Flag 를 설계해보자

**SurgGram**을 만들어가는 **PLT팀**과 유관부서들이 있습니다.

**PM팀**에서 **Core Version**`(v0.1.0)`을 정해주고 기능의 범위를 정해줍니다. (로그인기능을 예시로 설명합니다)

**PLT팀**은 정해진 **Core Version**에 맞게 구현 단계에 들어가게됩니다. 그렇게 `v0.1.0-dev1` 버전이 탄생합니다.

```tsx
export const SURGGRAM_VERSION_0_1_0_DEV1 = {
  로그인: FeatureFlag.onDevelopment({
    // 구현단계이므로 개발환경에서만 제공합니다.
    key: 'login',
    description: '로그인 기능',
  }),
} as const
```

`v0.1.0-dev1`의 피처플래그를 만듭니다. 여기서 확인할것은 세가지입니다. 현재 기능이 개발환경, 스테이징환경, 운영환경에 배포가 되어도 되는 상태인지 아닌지를 확인합니다.

`v0.1.0-dev1`는 구현단계이므로 **development**만 true로 만들어놓습니다.

`v0.1.0-dev1`에서 무언가 문제가있어 보완하기위해 `v0.1.0-dev2` 버전이 탄생합니다.

```tsx
// hutom-frontend/packages/feature-flag/surggram/v0.1.0-dev2.ts
export const SURGGRAM_VERSION_0_1_0_DEV2 = {
  ...SURGGRAM_VERSION_0_1_0_DEV1,
} as const
```

바로 아래 버전인 `v0.1.0-dev1` 의 피처플래그를 가져와서 구조분해할당해줍니다. 윗버전은 하위버전의 기능을 모두 포함해야하기때문입니다. 만약 하위버전의 기능중 변경되어야하는것이 있다라면 기능의 키의 값을 변경해주면됩니다. `v0.1.0-dev2` 에서는 `v0.1.0-dev1` 와 비교해 달라진게없으므로 그대로 가져갑니다.

`v0.1.0-dev2` 로 구현단계가 완료되었습니다. `v0.1.0-vnv1-build1` 버전이 탄생합니다.

```tsx
// hutom-frontend/packages/feature-flag/surggram/v0.1.0-vnv1-build1.ts
export const SURGGRAM_VERSION_0_1_0_VNV1_BUILD1 = {
  ...SURGGRAM_VERSION_0_1_0_DEV2,
  로그인: FeatureFlag.onStaging({
    // V&V 검증단계이므로 개발, 스테이징 환경에서만 제공합니다.
    key: 'login',
    description: '로그인 기능',
  }),
} as const
```

검증단계에 들어왔으니 스테이징환경에서 제공해야합니다. 그래서 로그인 피처플래그의 **staging** 속성을 true로 변경하였습니다.

1차 검증단계에서 수정이 필요해서 `v0.1.0-vnv1-build2` 버전을 만들어 변경합니다. 이후 다시 V&V팀에게 2차 검증을 요청하기 위해 `v0.1.0-vnv2-build3` 버전을 만들어서 제공합니다.

```tsx
// hutom-frontend/packages/feature-flag/surggram/v0.1.0-vnv2-build3.ts
export const SURGGRAM_VERSION_0_1_0_VNV2_BUILD3 = {
  ...SURGGRAM_VERSION_0_1_0_VNV1_BUILD2,
} as const
```

`v0.1.0-vnv2-build3` 의 로그인 피처플래그는 `v0.1.0-vnv1-build2` 에서 수정했을뿐 공개범위는 달라지지 않았으니 그대로 가져갑니다.

V&V의 2차 검증이 완료된후 버전을 릴리즈해야하는 상황이 왔습니다. 이제 `v0.1.0`을 릴리즈 합니다.

```tsx
// hutom-frontend/packages/feature-flag/surggram/v0.1.0.ts
export const SURGGRAM_VERSION_0_1_0 = {
  ...SURGGRAM_VERSION_0_1_0_VNV2_BUILD3,
  로그인: FeatureFlag.onProduction({
    // V&V 검증이 완료되었으니 개발, 스테이징, 운영환경 모두 제공!
    key: 'login',
    description: '로그인 기능',
  }),
} as const
```

`v0.1.0` 은 운영환경에서도 제공이 되어야합니다. 그러므로 로그인 기능의 **production** 속성을 true 로 변경해줍니다.

이렇게 버전에 따라 달라지는 공개범위에대한 피처플래그 설계는 완료되었습니다. 그렇다면 어떻게 써야하는지를 알아봐야합니다.

```tsx
// hutom-frontend/packages/feature-flag/surggram/index.ts
export const LAST_VERSION_FEATURE_FLAG = SURGGRAM_VERSION_0_1_0

// scope의 undefined는 Production에서 쿼리스트링이 없을 때를 대비한 default 값입니다.
export const IS_ENABLED_FEATURE_FLAG = (
  key: keyof typeof LAST_VERSION_FEATURE_FLAG,
  scope: 'development' | 'staging' | undefined
) => {
  const featureFlag = FeatureFlag.findByFeatureFlagKey(LAST_VERSION_FEATURE_FLAG, key)

  return FeatureFlag.isEnabledByScope(featureFlag, scope || 'production')
}

// hutom-frontend/apps/surggram/src/(pages)/login/page.tsx
export default function LoginPage({ searchParams }: Props) {
  if (!IS_ENABLED_FEATURE_FLAG('로그인', searchParams.scope)) {
    redirect('/')
  }

  return <div>로그인 페이지</div>
}
```

위 부터 아래로 한번 알아보겠습니다.

- `LAST_VERSION_FEATURE_FLAG`
  - development / staging / production 피처플래그의 정보가 담겨있는 최신화된 버전의 피처플래그입니다.
- `IS_ENABLED_FEATURE_FLAG`
  - `LAST_VERSION_FEATURE_FLAG` 의 특정 피처플래그가 특정 환경에서 제공 가능한 상태인지 확인하는 함수입니다.

이렇게 **@packages/feature-flag/surggram** 패키지에서 제공된 피처플래그 정보로 **@apps/surggram** 애플리케이션 패키지에서 기능의 제공 여부를 열고 닫아줍니다.

## TBD + Feature Flag

TBD는 Trunk 브랜치에서 협업하며, 긴 작업단위의 브랜치 생성을 피합니다. 그렇게 함으로서 협업 간의 충돌을 최소화하며 빌드가 실패하는 상태를 막습니다. 가장 중요한것은 **Trunk 브랜치**는 **항상 배포가 가능한 올바른 상태로 유지**하는것입니다.

이 말은 즉 **Trunk**라는 단일브랜치에서 개발환경 / 스테이징환경 / 프로덕션환경 모두 배포가 가능한 올바른 상태로 유지를 해야한다는 말이 됩니다.

그래서 피처플래그에 개발환경 / 스테이징환경 / 프로덕경환경에서 사용 가능한 상태인지를 체크하는 플래그를 심어줬던것입니다.

피처플래그 기능이 페이지단위에서 꺼져있다면 루트페이지로 리다이렉트하고, 컴포넌트단위에서 꺼져있다면 보이지않도록 정의하면됩니다.

이제 어떻게 지속적으로 제공하며 안정적으로 테스팅했는지 next.js 14버전 기준으로 소개하겠습니다.

우선 피처플래그에게 현재의 환경이 어느환경인지 어떻게 제공할지를 생각해보다가 환경변수로 Scope를 제공해주면 되겠다는 생각을했습니다. 그리고 런타임에 어떻게 프론트엔드 서버에 주입을 시킬지를 생각했습니다.

프론트엔드의 서버는 웹서버이므로 HTTP Request를 받습니다. 그리고 프론트엔드의 서버는 Next.js로 만들어져있습니다. 그래서 Next.js에서 제공하는 미들웨어를 사용하기로 하고 [문서](https://nextjs.org/docs/app/building-your-application/routing/middleware#use-cases)를 읽어보았습니다.

- **Path Rewriting**: Support A/B testing, feature rollouts, or legacy paths by dynamically rewriting paths to API routes or pages based on request properties
- **Feature Flagging**: Enable or disable features dynamically for seamless feature rollouts or testing.

훌륭합니다! 미들웨어에서 사용하는 방법이 유즈케이스라고 소개하고있고 이제 미들웨어를 제어하면됩니다.

```tsx
export async function middleware(req: NextRequest) {
  const url = req.nextUrl.clone()

  switch (env.SCOPE) {
    case 'development': {
      if (!url.searchParams.has('scope')) {
        url.searchParams.set('scope', 'development')
        return NextResponse.redirect(url)
      }
      break
    }

    case 'staging': {
      if (!url.searchParams.has('scope')) {
        url.searchParams.set('scope', 'staging')
        return NextResponse.redirect(url)
      }
      break
    }
    case 'production': {
      if (url.searchParams.has('scope')) {
        url.searchParams.delete('scope')
        return NextResponse.redirect(url)
      }
      break
    }
  }

  return NextResponse.next()
}
```

Next.js의 미들웨어는 요청이 완료되기 전에 코드를 실행할 수 있습니다. 요청에 따라 요청 또는 응답 헤더를 재작성하거나 리다이렉션 혹은 수정할 수 있습니다.

그렇게 개발환경과 스테이징환경에서는 스코프를 쿼리파람즈로 제공하고, 운영환경에서는 실제 유저가 쿼리파람즈가 없이도 사용이 가능하도록 스코프를 제거한채 제공합니다.

![](/static/images/DEV/hutom-monorepo/query-param-feature-flag.gif)

개발환경에서 basePath로 접근한경우 자동으로 스코프가 쿼리파람즈로 붙게됩니다! 이렇게 피처플래그를 서버에게 제공해주고있는것까지 완료했습니다.

그러면 이제 안정적으로 피처플래그가 동작하고있는지 테스트하기위해 테스트코드를 작성하도록 합니다.

```tsx
// 현재 개발중인 버전
export const SURGGRAM_VERSION_0_1_0_DEV1 = {
  로그인: FeatureFlag.onDevelopment({
    key: '로그인',
    description: '로그인 기능',
  }),
} as const

// 로그인 페이지
const LoginPage = ({ searchParams }: Props) => {
  if (!IS_ENABLED_FEATURE_FLAG('로그인', searchParams.scope)) {
    redirect(`/`)
  }

  return <div>로그인 페이지</div>
}
```

로그인 페이지의 기능제공범위는 아래와같다고 상황을 가정해 봅니다.

- 개발환경 : `**On**`
- 스테이징환경 : `**Off**`
- 프로덕션환경 : `**Off**`

이를 시나리오로 E2E테스트 도구인 Cypress의 도움을 받아 테스트 케이스를 작성해 봅니다. 피처플래그를 테스트하는 E2E테스트는 패키지의 `**/cypress/e2e/feature-flags**` 폴더에서 관리하도록 해두었습니다.

```tsx
describe('Login FeatureFlag 테스트', () => {
  it('development 환경에서는 화면이 보인다.', () => {
    cy.visit('/login?scope=development')

    cy.location().should((location) => {
      expect(location.pathname).to.eq('/login')
    })
  })

  it('staging 환경에서는 랜딩페이지로 리다이렉트된다.', () => {
    cy.visit('/login?scope=staging')

    cy.location().should((location) => {
      expect(location.pathname).to.eq('/')
    })
  })

  it('production 환경에서는 랜딩페이지로 리다이렉트된다.', () => {
    cy.visit('/login')

    cy.location().should((location) => {
      expect(location.pathname).to.eq('/')
    })
  })
})
```

각 케이스에맞게 시나리오대로 케이스를 작성하였습니다.

![](/static/images/DEV/hutom-monorepo/e2e.gif)

# 참고자료

- [버전관리 시스템 전략](https://www.notion.so/474f3d9024cf4d2fbdf365e6ff27eeeb?pvs=21)
- [Yarn berry](https://www.notion.so/Yarn-berry-4128ad841a5849ea9d39186fe888a0d2?pvs=21)
- [node_modules로부터 우리를 구원해 줄 Yarn Berry](https://toss.tech/article/node-modules-and-yarn-berry)
- [모노레포 적용부터 yarn berry까지](https://blog.hwahae.co.kr/all/tech/11962)
- [Symbolic Linked node_modules Structure](https://pnpm.io/next/symlinked-node-modules-structure)
- [모던 프론트엔드 프로젝트 구성 기법 - 모노레포 도구 편](https://d2.naver.com/helloworld/7553804)
- [TurboRepo와 캐싱 - 커스텀 리모트 캐시 적용하기](https://programming119.tistory.com/280)
